{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":799923,"sourceType":"datasetVersion","datasetId":374}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertTokenizerFast, BertForQuestionAnswering, AdamW\nfrom tqdm.notebook import tqdm\nimport pandas as pd\n\n# Load the pre-trained BERT tokenizer (using the Fast version)\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n\n# Example DataLoader for the SQuAD dataset\nclass SQuADDataset(torch.utils.data.Dataset):\n    def __init__(self, dataframe, tokenizer):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        question = self.data.iloc[idx]['question']\n        context = self.data.iloc[idx]['context']\n        answer_text = self.data.iloc[idx]['answer_text']\n        answer_start = self.data.iloc[idx]['answer_start']\n\n        # Use the fast tokenizer to encode the input with offset mapping\n        encoded_input = self.tokenizer.encode_plus(\n            question,\n            context,\n            return_offsets_mapping=True,  # Only available in the Fast tokenizer\n            padding='max_length',\n            truncation=True,\n            max_length=512\n        )\n\n        # Find the start and end token indices within the encoded sequence\n        offset_mapping = encoded_input['offset_mapping']\n        input_ids = encoded_input['input_ids']\n        attention_mask = encoded_input['attention_mask']\n        token_type_ids = encoded_input['token_type_ids']\n\n        # Convert character positions to token positions\n        start_positions = None\n        end_positions = None\n        for i, (start, end) in enumerate(offset_mapping):\n            if start <= answer_start < end:\n                start_positions = i\n            if start < answer_start + len(answer_text) <= end:\n                end_positions = i\n\n        return {\n            'input_ids': torch.tensor(input_ids, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'start_positions': torch.tensor(start_positions, dtype=torch.long),\n            'end_positions': torch.tensor(end_positions, dtype=torch.long)\n        }\n\n# Load your data into a pandas DataFrame\ntrain_data = pd.DataFrame({\n    'question': [\"What is the capital of France?\", \"Who wrote Hamlet?\"],\n    'context': [\"Paris is the capital of France.\", \"Shakespeare wrote Hamlet.\"],\n    'answer_text': [\"Paris\", \"Shakespeare\"],\n    'answer_start': [0, 0]\n})\n\n# Create dataset and dataloader\ntrain_dataset = SQuADDataset(train_data, tokenizer)\ntrain_dataloader = DataLoader(train_dataset, batch_size=2)\n\n# Load pre-trained BERT model for Question Answering\nmodel = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n\n# Define optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\nnum_epochs = 3\nmodel.train()\n\nfor epoch in range(num_epochs):\n    train_loss = 0\n    train_steps = 0\n    \n    # Training loop with tqdm progress bar\n    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n        input_ids = batch['input_ids']\n        attention_mask = batch['attention_mask']\n        token_type_ids = batch['token_type_ids']\n        start_positions = batch['start_positions']\n        end_positions = batch['end_positions']\n\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            start_positions=start_positions,\n            end_positions=end_positions\n        )\n        \n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        train_steps += 1\n\n    avg_train_loss = train_loss / train_steps\n    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_train_loss:.4f}\")\n\nprint(\"Training complete.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-09-04T05:55:27.082768Z","iopub.execute_input":"2024-09-04T05:55:27.083585Z","iopub.status.idle":"2024-09-04T05:55:38.931526Z","shell.execute_reply.started":"2024-09-04T05:55:27.083545Z","shell.execute_reply":"2024-09-04T05:55:38.930241Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nA parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\nA parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\nSome weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74417844d8f94176960772bf68640ecc"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/3 - Loss: 6.0894\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/3:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a83f8143c2bd4c86b45c1025e1753400"}},"metadata":{}},{"name":"stdout","text":"Epoch 2/3 - Loss: 5.0559\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/3:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66efb3dd82464e8e80a44a14ff0c556f"}},"metadata":{}},{"name":"stdout","text":"Epoch 3/3 - Loss: 4.2274\nTraining complete.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.save_pretrained('./saved_model')\ntokenizer.save_pretrained('./saved_tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-09-04T05:55:42.139820Z","iopub.execute_input":"2024-09-04T05:55:42.140298Z","iopub.status.idle":"2024-09-04T05:55:42.961200Z","shell.execute_reply.started":"2024-09-04T05:55:42.140251Z","shell.execute_reply":"2024-09-04T05:55:42.959897Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"('./saved_tokenizer/tokenizer_config.json',\n './saved_tokenizer/special_tokens_map.json',\n './saved_tokenizer/vocab.txt',\n './saved_tokenizer/added_tokens.json',\n './saved_tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"def predict(question, context, model, tokenizer):\n    inputs = tokenizer.encode_plus(question, context, return_tensors='pt', max_length=512, truncation=True)\n    input_ids = inputs['input_ids']\n    attention_mask = inputs['attention_mask']\n\n    outputs = model(input_ids, attention_mask=attention_mask)\n    start_scores = outputs.start_logits\n    end_scores = outputs.end_logits\n\n    # Get the most likely start and end tokens\n    start_index = torch.argmax(start_scores)\n    end_index = torch.argmax(end_scores) + 1\n\n    answer = tokenizer.convert_tokens_to_string(\n        tokenizer.convert_ids_to_tokens(input_ids[0][start_index:end_index])\n    )\n    return answer\n\nquestion = \"What is the capital of France?\"\ncontext = \"Paris is the capital of France.\"\nanswer = predict(question, context, model, tokenizer)\nprint(f\"Predicted answer: {answer}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T05:55:42.963576Z","iopub.execute_input":"2024-09-04T05:55:42.963982Z","iopub.status.idle":"2024-09-04T05:55:43.077731Z","shell.execute_reply.started":"2024-09-04T05:55:42.963948Z","shell.execute_reply":"2024-09-04T05:55:43.076776Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Predicted answer: paris\n","output_type":"stream"}]},{"cell_type":"code","source":"def token_iou(pred_tokens, true_tokens):\n    \"\"\"\n    Calculate the token-level Intersection over Union (IoU) between predicted and true tokens.\n\n    Args:\n        pred_tokens (list): List of predicted tokens.\n        true_tokens (list): List of true tokens.\n\n    Returns:\n        float: The IoU score.\n    \"\"\"\n    set_pred = set(pred_tokens)\n    set_true = set(true_tokens)\n    intersection = set_pred.intersection(set_true)\n    union = set_pred.union(set_true)\n    iou = len(intersection) / len(union) if len(union) > 0 else 0.0\n    return iou\n","metadata":{"execution":{"iopub.status.busy":"2024-09-04T05:55:43.627260Z","iopub.execute_input":"2024-09-04T05:55:43.627695Z","iopub.status.idle":"2024-09-04T05:55:43.633977Z","shell.execute_reply.started":"2024-09-04T05:55:43.627657Z","shell.execute_reply":"2024-09-04T05:55:43.632946Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def evaluate_model(model, dataloader, tokenizer):\n    model.eval()\n    total_iou = 0\n    num_batches = len(dataloader)\n\n    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n        with torch.no_grad():\n            input_ids = torch.stack(batch['input_ids']).squeeze(1).to(model.device)\n            attention_mask = torch.stack(batch['attention_mask']).squeeze(1).to(model.device)\n            start_positions = torch.tensor(batch['start_positions']).to(model.device)\n            end_positions = torch.tensor(batch['end_positions']).to(model.device)\n\n            outputs = model(input_ids, attention_mask=attention_mask)\n            start_logits = outputs.start_logits\n            end_logits = outputs.end_logits\n\n            # Get the most likely start and end tokens\n            start_preds = torch.argmax(start_logits, dim=1)\n            end_preds = torch.argmax(end_logits, dim=1)\n\n            for i in range(len(input_ids)):\n                pred_tokens = tokenizer.convert_ids_to_tokens(input_ids[i][start_preds[i]:end_preds[i]+1])\n                true_tokens = tokenizer.convert_ids_to_tokens(input_ids[i][start_positions[i]:end_positions[i]+1])\n\n                iou = token_iou(pred_tokens, true_tokens)\n                total_iou += iou\n\n    avg_iou = total_iou / num_batches\n    return avg_iou","metadata":{"execution":{"iopub.status.busy":"2024-09-04T05:55:44.549768Z","iopub.execute_input":"2024-09-04T05:55:44.550549Z","iopub.status.idle":"2024-09-04T05:55:44.560536Z","shell.execute_reply.started":"2024-09-04T05:55:44.550504Z","shell.execute_reply":"2024-09-04T05:55:44.559494Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}